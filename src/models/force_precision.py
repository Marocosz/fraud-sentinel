import pandas as pd
import numpy as np
import joblib
import sys
import logging
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.metrics import precision_recall_curve, classification_report, confusion_matrix, accuracy_score, roc_auc_score, f1_score

# ==============================================================================
# ARQUIVO: force_precision.py
#
# OBJETIVO:
#   Ajustar o ponto de operaÃ§Ã£o (Threshold Tuning) do modelo XGBoost para atingir
#   uma Precision (PrecisÃ£o) alvo definida pelo negÃ³cio.
#
# CONTEXTO DE NEGÃ“CIO:
#   Em fraude, Precision baixa = Muitos clientes legÃ­timos bloqueados (Falsos Positivos).
#   Isso gera atrito, reclamaÃ§Ãµes e custo operacional (equipe de revisÃ£o manual).
#   Este script permite dizer: "Quero garantir que pelo menos 20% dos alertas sejam reais".
#
# METODOLOGIA ACADÃŠMICA:
#   Utiliza a curva Precision-Recall (PR Curve) para varrer todos os limiares possÃ­veis
#   e encontrar matematicamente o menor threshold que satisfaz a restriÃ§Ã£o:
#   Precision >= Target_Precision.
#
# OUTPUTS:
#   - RelatÃ³rio de ClassificaÃ§Ã£o e Matriz de ConfusÃ£o ajustados.
#   - GrÃ¡fico da Curva Precision-Recall com o ponto escolhido.
#   - Sobrescreve 'models/xgb_threshold.txt' com o novo valor otimizado.
# ==============================================================================

# ConfiguraÃ§Ã£o de Caminhos
PROJECT_ROOT = Path(__file__).resolve().parents[2]
sys.path.append(str(PROJECT_ROOT))

# Imports do Projeto
from src.config import PROCESSED_DATA_DIR, MODELS_DIR, FIGURES_DIR

# ConfiguraÃ§Ã£o de Logs
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [PRECISION_FORCE] - %(message)s')
logger = logging.getLogger(__name__)

def plot_precision_recall_vs_threshold(precisions, recalls, thresholds, selected_threshold, selected_precision, selected_recall):
    """
    Gera grÃ¡fico profissional mostrando o trade-off Precision vs Recall para diferentes thresholds.
    """
    plt.figure(figsize=(10, 6))
    plt.title("Precision-Recall vs Threshold Trade-off")
    plt.plot(thresholds, precisions[:-1], "b--", label="Precision")
    plt.plot(thresholds, recalls[:-1], "g-", label="Recall")
    
    # Marca o ponto escolhido
    plt.axvline(x=selected_threshold, color='r', linestyle=':', label=f'Selected Threshold ({selected_threshold:.4f})')
    plt.scatter(selected_threshold, selected_precision, color='blue', s=100, zorder=5)
    plt.scatter(selected_threshold, selected_recall, color='green', s=100, zorder=5)
    
    plt.xlabel("Threshold")
    plt.ylabel("Score")
    plt.legend(loc="best")
    plt.grid(True, alpha=0.3)
    
    output_path = FIGURES_DIR / "precision_optimization_curve.png"
    plt.savefig(output_path)
    logger.info(f"ðŸ“Š GrÃ¡fico de trade-off salvo em: {output_path}")
    plt.close()

def enforce_precision_target(target_precision=0.20, model_filename="xgb_best_model.pkl"):
    """
    Encontra e aplica o threshold que garante a precisÃ£o mÃ­nima desejada.
    
    Args:
        target_precision (float): Alvo de precisÃ£o (ex: 0.20 para 20%).
        model_filename (str): Nome do arquivo do modelo a ser carregado.
    """
    logger.info(f"ðŸš€ Iniciando otimizaÃ§Ã£o de threshold. Alvo: Precision >= {target_precision*100}%")
    
    # 1. Carregar Dados de Teste (Blind Set)
    try:
        X_test = pd.read_csv(PROCESSED_DATA_DIR / "X_test.csv")
        y_test = pd.read_csv(PROCESSED_DATA_DIR / "y_test.csv").values.ravel()
        model_path = MODELS_DIR / model_filename
        
        if not model_path.exists():
            raise FileNotFoundError(f"Modelo {model_filename} nÃ£o encontrado em {MODELS_DIR}")
            
        model = joblib.load(model_path)
        logger.info(f"âœ… Modelo carregado: {model_filename}")
        
    except Exception as e:
        logger.error(f"âŒ Erro ao carregar artefatos: {e}")
        return

    # 2. Obter Probabilidades (Score de Risco)
    logger.info("ðŸ”® Calculando probabilidades de fraude no conjunto de teste...")
    if hasattr(model, "predict_proba"):
        y_proba = model.predict_proba(X_test)[:, 1]
    else:
        # Fallback para modelos sem predict_proba (ex: SVM linear), embora raro neste projeto
        y_proba = model.decision_function(X_test)
        # Normaliza com Sigmoid se necessÃ¡rio, mas aqui assumimos proba direta

    # 3. Calcular Curva Precision-Recall
    precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba)

    # 4. Busca do Menor Threshold que satisfaz a condiÃ§Ã£o
    # precisions e recalls tÃªm tamanho n_thresholds + 1 (o Ãºltimo Ã© 1.0 e 0.0)
    # thresholds tem tamanho n_thresholds
    
    found_idx = -1
    
    # Varredura eficiente
    # Buscamos o primeiro Ã­ndice onde precision >= target
    # Nota: thresholds sÃ£o crescentes? NÃ£o necessariamente no retorno do sklearn, mas geralmente sim.
    # O sklearn retorna thresholds crescentes em decision_function, mas para proba pode variar.
    # Vamos garantir iterando sobre a curva ordenada.
    
    # Criamos um DataFrame para analisar melhor
    pr_df = pd.DataFrame({
        'threshold': thresholds, 
        'precision': precisions[:-1], 
        'recall': recalls[:-1]
    })
    
    # Filtra apenas linhas que atendem o critÃ©rio
    candidates = pr_df[pr_df['precision'] >= target_precision]
    
    if candidates.empty:
        max_prec_achievable = np.max(precisions)
        logger.error(f"âŒ IMPOSSÃVEL atingir {target_precision*100}% de precisÃ£o com este modelo.")
        logger.error(f"   A precisÃ£o mÃ¡xima teÃ³rica alcanÃ§Ã¡vel Ã© {max_prec_achievable*100:.2f}%")
        return

    # Entre os candidatos, escolhemos o que tem maior Recall (para nÃ£o perder fraude Ã  toa)
    # Geralmente isso equivale ao MENOR threshold que bate a precisÃ£o.
    best_candidate = candidates.loc[candidates['recall'].idxmax()]
    
    final_threshold = best_candidate['threshold']
    final_precision = best_candidate['precision']
    final_recall = best_candidate['recall']

    logger.info("\nâœ… PONTO DE OPERAÃ‡ÃƒO Ã“TIMO ENCONTRADO!")
    logger.info(f"   ðŸŽ¯ Threshold de Corte: {final_threshold:.4f}")
    logger.info(f"   ðŸ’Ž Precision Esperada: {final_precision*100:.2f}% (Meta: {target_precision*100}%)")
    logger.info(f"   ðŸ” Recall Resultante:  {final_recall*100:.2f}%")

    # 5. ValidaÃ§Ã£o (Prova Real)
    logger.info("\nðŸ“Š Aplicando novo corte nos dados de teste...")
    y_pred_new = (y_proba >= final_threshold).astype(int)
    
    # MÃ©tricas Globais
    acc = accuracy_score(y_test, y_pred_new)
    auc = roc_auc_score(y_test, y_proba) # AUC independe do threshold
    f1 = f1_score(y_test, y_pred_new)
    
    print("\n" + "="*60)
    print("RELATÃ“RIO DE PERFORMANCE (Precision-Oriented)")
    print("="*60)
    print(classification_report(y_test, y_pred_new))
    
    # Matriz de ConfusÃ£o Customizada
    cm = confusion_matrix(y_test, y_pred_new)
    tn, fp, fn, tp = cm.ravel()
    
    total_samples = len(y_test)
    total_fraud = fn + tp
    total_legit = tn + fp
    
    print("\n--- MATRIZ DE CONFUSÃƒO ANALÃTICA ---")
    print(f"ðŸŸ¢ LegÃ­timos Aprovados (TN): {tn} ({(tn/total_legit)*100:.1f}%)")
    print(f"ðŸ”´ LegÃ­timos Bloqueados (FP): {fp} ({(fp/total_legit)*100:.1f}%) -> CUSTO DE ATRITO")
    print(f"âš ï¸ Fraudes Detectadas   (TP): {tp} ({(tp/total_fraud)*100:.1f}%) -> RECALL")
    print(f"ðŸ’¸ Fraudes Perdidas     (FN): {fn} ({(fn/total_fraud)*100:.1f}%) -> PREJUÃZO FINANCEIRO")
    print("-" * 60)
    
    # Plotar grÃ¡fico
    plot_precision_recall_vs_threshold(precisions, recalls, thresholds, final_threshold, final_precision, final_recall)

    # 6. PersistÃªncia
    # Identifica nome base do modelo para salvar o threshold correto
    model_base_name = model_filename.split("_")[0] # ex: 'xgb' from 'xgb_best_model.pkl'
    threshold_file = MODELS_DIR / f"{model_base_name}_threshold.txt"
    
    with open(threshold_file, "w") as f:
        f.write(str(final_threshold))
        
    logger.info(f"ðŸ’¾ Novo Threshold salvo em: {threshold_file}")
    logger.info("   O sistema de inferÃªncia (predict_model.py) passarÃ¡ a usar este valor automaticamente.")

if __name__ == "__main__":
    # Permite customizar via argumento simples ou usa default 20%
    if len(sys.argv) > 1:
        try:
            target = float(sys.argv[1])
            enforce_precision_target(target_precision=target)
        except ValueError:
            print("Uso: python force_precision.py [target_precision_float]")
    else:
        enforce_precision_target(target_precision=0.20)