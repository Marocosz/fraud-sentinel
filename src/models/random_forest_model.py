import pandas as pd
import numpy as np
import joblib
import sys
import logging
import warnings
import contextlib
import json
import datetime
from pathlib import Path
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.pipeline import Pipeline
from sklearn.metrics import precision_recall_curve

try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False

# ==============================================================================
# ARQUIVO: random_forest_model.py
#
# OBJETIVO:
#   Treinar e otimizar o modelo de Random Forest.
#   Este script foca na otimiza√ß√£o de hiperpar√¢metros especificamente para este algoritmo.
#
# PARTE DO SISTEMA:
#   M√≥dulo de Treinamento e Otimiza√ß√£o (Model Training Stage).
#
# RESPONSABILIDADES:
#   - Carregar o dataset de treino (X_train, y_train).
#   - Definir o espa√ßo de busca de hiperpar√¢metros (Grid Search).
#   - Executar a busca com valida√ß√£o cruzada para garantir robustez.
#   - Persistir o melhor modelo encontrado (.pkl).
#   - Registrar logs detalhados do processo de treinamento.
#
# COMUNICA√á√ÉO:
#   - L√™: data/processed/X_train.csv, y_train.csv
#   - Escreve: models/rf_best_model.pkl
#   - Escreve: models/rf_best_model_params.txt
# ==============================================================================

# Ignora avisos de deprecia√ß√£o do Scikit-Learn e pkg_resources
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning, module="sklearn")
warnings.filterwarnings("ignore", message=".*pkg_resources.*")

# Configura√ß√£o de Caminhos
PROJECT_ROOT = Path(__file__).resolve().parents[2]
sys.path.append(str(PROJECT_ROOT))

# Imports do Projeto
from src.config import PROCESSED_DATA_DIR, MODELS_DIR, RANDOM_STATE, REPORTS_DIR
from src.features.build_features import build_pipeline

# Configura√ß√£o de Logs (Profissionalismo)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# ==============================================================================
# CONFIGURA√á√ÉO DO MODELO (PAR√ÇMETROS)
# ==============================================================================
MODEL_CONFIG = {
    # Modelo Base
    "model_class": RandomForestClassifier,
    "model_params": {
        "n_estimators": 100,
        "class_weight": "balanced", # [MODIFICA√á√ÉO] Reativado para compensar a falta do SMOTE
        "n_jobs": -1,
        "random_state": RANDOM_STATE
    },
    
    # Estrat√©gia de Oversampling
    "smote_strategy": None,
    
    # Valida√ß√£o Cruzada
    "cv_folds": 3,                  # R√°pido e suficiente para grandes volumes
    
    # Espa√ßo de Busca (Grid Search)
    "param_grid": {
        'model__n_estimators': [100, 200],
        'model__max_depth': [10, 20, None],
        'model__min_samples_split': [2, 5]
    },
    
    # Configura√ß√£o de Execu√ß√£o
    "n_jobs": 1,                    # GridSearch jobs (o RF j√° usa jobs internos)
    "verbose": 2
}

def train_random_forest():
    """
    Treina o modelo de Random Forest com otimiza√ß√£o completa de hiperpar√¢metros.
    """
    
    # Gerar ID √∫nico para o experimento (Timestamp)
    run_id = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    logger.info(f"üöÄ Iniciando Pipeline de Treinamento Random Forest (Run ID: {run_id})...")
    logger.info(f"‚ÑπÔ∏è  Configura√ß√£o carregada: SMOTE={MODEL_CONFIG['smote_strategy']}")
    
    # -------------------------------------------------------------------------
    # 1. CARGA DE DADOS
    # -------------------------------------------------------------------------
    X_train_path = PROCESSED_DATA_DIR / "X_train.csv"
    y_train_path = PROCESSED_DATA_DIR / "y_train.csv"
    
    if not X_train_path.exists():
        logger.error("‚ùå Arquivos de treino n√£o encontrados. Rode 'make_dataset.py' primeiro.")
        return

    logger.info("üìÇ Carregando dados de treino...")
    X_train = pd.read_csv(X_train_path)
    y_train = pd.read_csv(y_train_path).values.ravel()
    
    logger.info(f"   Dimens√µes: {X_train.shape[0]} amostras, {X_train.shape[1]} features.")

    # -------------------------------------------------------------------------
    # 2. DEFINI√á√ÉO DO PIPELINE (EDA-DRIVEN)
    # -------------------------------------------------------------------------
    # Pipeline de 3 etapas: EDAFeatureEngineer -> ColumnTransformer -> Modelo
    clf = MODEL_CONFIG["model_class"](**MODEL_CONFIG["model_params"])
    
    logger.info("‚ùå SMOTE Desativado. Usando class_weight='balanced'.")
    logger.info("üî¨ Aplicando Feature Engineering baseado na EDA.")
    pipeline = build_pipeline(X_train, clf)
    
    # -------------------------------------------------------------------------
    # 3. ESPA√áO DE HIPERPAR√ÇMETROS (Grid Search)
    # -------------------------------------------------------------------------
    cv = StratifiedKFold(n_splits=MODEL_CONFIG["cv_folds"], shuffle=True, random_state=RANDOM_STATE)
    
    grid_search = GridSearchCV(
        estimator=pipeline,
        param_grid=MODEL_CONFIG["param_grid"],
        scoring='roc_auc', 
        cv=cv,
        n_jobs=MODEL_CONFIG["n_jobs"],
        verbose=MODEL_CONFIG["verbose"]
    )
    
    # -------------------------------------------------------------------------
    # 4. TREINAMENTO E OTIMIZA√á√ÉO
    # -------------------------------------------------------------------------
    logger.info("‚öôÔ∏è  Otimizando Hiperpar√¢metros (GridSearchCV)...")
    logger.info(f"   Espa√ßo de busca: {MODEL_CONFIG['param_grid']}")
    
    print(f"\n‚ö° Iniciando treinamento...")
    grid_search.fit(X_train, y_train)
    
    # -------------------------------------------------------------------------
    # 5. RESULTADOS E PERSIST√äNCIA
    # -------------------------------------------------------------------------
    best_model = grid_search.best_estimator_
    best_params = grid_search.best_params_
    best_score = grid_search.best_score_
    
    logger.info("‚úÖ Treinamento Conclu√≠do!")
    logger.info(f"üèÜ Melhor ROC-AUC M√©dio: {best_score:.4f}")
    logger.info(f"üîß Melhores Par√¢metros: {best_params}")
    
    # 1. Salvar Modelo Final (Vers√£o Atual/Latest para o sistema usar)
    latest_model_path = MODELS_DIR / "rf_best_model.pkl"
    joblib.dump(best_model, latest_model_path)
    
    # 2. Salvar Modelo Versionado (Hist√≥rico)
    versioned_model_path = MODELS_DIR / f"model_rf_{run_id}.pkl"
    joblib.dump(best_model, versioned_model_path)
    
    logger.info(f"üíæ Modelo salvo em: {latest_model_path}")
    logger.info(f"üíæ C√≥pia de hist√≥rico salva em: {versioned_model_path}")
    
    # -------------------------------------------------------------------------
    # 6. THRESHOLD TUNING (F1-Score Maximization)
    # -------------------------------------------------------------------------
    logger.info("‚öñÔ∏è  Calculando Best Threshold...")
    
    # Previs√µes de probabilidade no treino
    y_train_proba = best_model.predict_proba(X_train)[:, 1]
    
    precisions, recalls, thresholds = precision_recall_curve(y_train, y_train_proba)
    
    # Calcula F1 para cada threshold
    # Adicionamos epsilon para evitar divis√£o por zero
    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)
    best_idx = np.argmax(f1_scores)
    best_threshold = thresholds[best_idx]
    best_f1 = f1_scores[best_idx]
    
    logger.info(f"üéØ Melhor Threshold: {best_threshold:.4f} (F1 esperado: {best_f1:.4f})")
    
    # Salvar threshold
    with open(MODELS_DIR / "rf_threshold.txt", "w") as f:
        f.write(str(best_threshold))

    # 3. Registrar Experimento no Log (JSON)
    experiment_data = {
        "run_id": run_id,
        "timestamp": datetime.datetime.now().isoformat(),
        "model_type": MODEL_CONFIG["model_class"].__name__,
        "smote_strategy": MODEL_CONFIG["smote_strategy"],
        "best_params": best_params,
        "best_cv_score": best_score,
        "best_threshold": float(best_threshold),
        "model_path": str(versioned_model_path.name)
    }
    
    experiments_log_path = REPORTS_DIR / "experiments_log.json"
    
    # L√™ o log existente ou cria lista vazia
    if experiments_log_path.exists():
        with open(experiments_log_path, "r") as f:
            try:
                history = json.load(f)
            except json.JSONDecodeError:
                history = []
    else:
        history = []
        
    history.append(experiment_data)
    
    with open(experiments_log_path, "w") as f:
        json.dump(history, f, indent=4)
        
    logger.info(f"üìù Experimento registrado em: {experiments_log_path}")
    
    # Salvar Relat√≥rio Simples
    with open(MODELS_DIR / "rf_best_model_params.txt", "w") as f:
        f.write(f"Run ID: {run_id}\n")
        f.write(f"Best ROC-AUC: {best_score:.4f}\n")
        f.write(f"Params: {best_params}\n")

if __name__ == "__main__":
    train_random_forest()
