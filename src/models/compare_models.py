import sys
import warnings
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.model_selection import StratifiedKFold, cross_validate
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier, HistGradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE

# ==============================================================================
# ARQUIVO: compare_models.py
#
# OBJETIVO:
#   Executar um torneio de compara√ß√£o (benchmark) entre m√∫ltiplos algoritmos de Machine Learning.
#   O script treina cada modelo usando valida√ß√£o cruzada (Cross-Validation) e seleciona o melhor
#   com base em m√©tricas chave como ROC-AUC, Recall e F1-Score.
#
# PARTE DO SISTEMA:
#   M√≥dulo de Sele√ß√£o de Modelos (Model Selection Stage).
#
# RESPONSABILIDADES:
#   - Carregar o dataset de treino processado (X_train.csv, y_train.csv).
#   - Aplicar amostragem estratificada para acelerar a compara√ß√£o inicial (evitar horas de treino em 1M linhas).
#   - Definir uma lista de competidores (LogReg, Random Forest, XGBoost, etc).
#   - Garantir que o pr√©-processamento (SMOTE, Scaler) ocorra DENTRO de cada fold da valida√ß√£o cruzada (preven√ß√£o de Data Leakage).
#   - Exportar resultados em CSV (persist√™ncia) e TXT (relat√≥rio executivo).
#   - Gerar gr√°ficos comparativos para facilitar a decis√£o visual.
#
# COMUNICA√á√ÉO:
#   - L√™: data/processed/X_train.csv, y_train.csv
#   - Escreve: reports/data/models_comparison_results.csv (Tabela de m√©tricas)
#   - Escreve: reports/model_comparison_report.txt (Relat√≥rio textual)
#   - Escreve: reports/figures/model_comparison_metrics.png (Gr√°fico de barras)
#
# DEPEND√äNCIAS EXTERNAS:
#   - Scikit-Learn (Pipelines, Models)
#   - Imbalanced-Learn (SMOTE, ImbPipeline)
#   - XGBoost / LightGBM (Gradient Boosting otimizado)
# ==============================================================================

# Adiciona raiz ao path para garantir que imports do pacote 'src' funcionem
PROJECT_ROOT = Path(__file__).resolve().parents[2]
sys.path.append(str(PROJECT_ROOT))

# Tenta importar configura√ß√µes centralizadas; define fallback para execu√ß√£o isolada
try:
    from src.config import PROCESSED_DATA_DIR, RANDOM_STATE, FIGURES_DIR, REPORTS_DIR
except ImportError:
    # Caminhos padr√£o caso o script seja executado fora do contexto do pacote principal
    PROCESSED_DATA_DIR = PROJECT_ROOT / "data" / "processed"
    FIGURES_DIR = PROJECT_ROOT / "reports" / "figures"
    REPORTS_DIR = PROJECT_ROOT / "reports"
    RANDOM_STATE = 42

from src.features.build_features import get_preprocessor, EDAFeatureEngineer

# Configura√ß√µes Globais e Constantes
# Filtra warning espec√≠fico do LightGBM quando usado em Pipeline do Scikit-Learn
warnings.filterwarnings("ignore", category=UserWarning, message=".*X does not have valid feature names, but LGBMClassifier was fitted with feature names.*")

SAMPLE_SIZE = 50000  # Tamanho da amostra: 50k √© estatisticamente suficiente para rankear algoritmos
CV_FOLDS = 5         # N√∫mero de folds: 5 garante robustez estat√≠stica sem demorar demais
COMPARISON_DATA_DIR = REPORTS_DIR / "data"
COMPARISON_REPORT_FILE = REPORTS_DIR / "model_comparison_report.txt"

# Garante que os diret√≥rios de sa√≠da existam
COMPARISON_DATA_DIR.mkdir(parents=True, exist_ok=True)

def compare_algorithms():
    """
    Fun√ß√£o principal que orquestra todo o benchmark de modelos.
    """
    print(f"ü•ä INICIANDO TORNEIO DE MODELOS (Amostra: {SAMPLE_SIZE} linhas)")
    
    # --------------------------------------------------------------------------
    # 1. CARGA DE DADOS
    # Carrega os dados processados que foram gerados na etapa de Feature Engineering.
    # --------------------------------------------------------------------------
    try:
        X = pd.read_csv(PROCESSED_DATA_DIR / "X_train.csv")
        # Garante que y seja um array 1D (vetor), necess√°rio para o scikit-learn
        y = pd.read_csv(PROCESSED_DATA_DIR / "y_train.csv").values.ravel()
    except FileNotFoundError:
        print("‚ùå Erro: Arquivos de treino n√£o encontrados. Rode 'python main.py --step split'.")
        return

    # --------------------------------------------------------------------------
    # 2. AMOSTRAGEM ESTRATIFICADA
    # Reduz o tamanho do dataset para agilizar o benchmark inicial.
    # "Estratificada" significa que mantemos a mesma % de fraudes original na amostra.
    # --------------------------------------------------------------------------
    if len(X) > SAMPLE_SIZE:
        print(f"‚úÇÔ∏è Reduzindo dataset para {SAMPLE_SIZE} inst√¢ncias (mantendo estratifica√ß√£o)...")
        from sklearn.model_selection import train_test_split
        X_sample, _, y_sample, _ = train_test_split(
            X, y, train_size=SAMPLE_SIZE, stratify=y, random_state=RANDOM_STATE
        )
    else:
        # Se o dataset for pequeno, usamos ele inteiro
        X_sample, y_sample = X, y

    # --------------------------------------------------------------------------
    # 3. DEFINI√á√ÉO DOS COMPETIDORES
    # Lista de tuplas (Nome, Inst√¢ncia do Modelo).
    # Usamos par√¢metros b√°sicos + balanceamento de classes onde poss√≠vel.
    # --------------------------------------------------------------------------
    models = [
        # Regress√£o Log√≠stica: Baseline linear (simples e interpret√°vel)
        ('LogReg', LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RANDOM_STATE)),
        
        # √Årvore de Decis√£o: Baseline n√£o-linear (captura regras if-else simples)
        ('DecisionTree', DecisionTreeClassifier(class_weight='balanced', random_state=RANDOM_STATE)),
        
        # Random Forest: Ensemble robusto (reduz vari√¢ncia, bom baseline forte)
        ('RandomForest', RandomForestClassifier(n_estimators=50, class_weight='balanced', n_jobs=-1, random_state=RANDOM_STATE)),
        
        # Gradient Boosting (Sklearn): Boosting padr√£o (reduz vi√©s)
        ('GradientBoosting', GradientBoostingClassifier(n_estimators=50, random_state=RANDOM_STATE)),

        # Histogram-based Gradient Boosting (Sklearn): Inspirado no LightGBM, muito mais r√°pido que o padr√£o
        ('HistGradientBoosting', HistGradientBoostingClassifier(random_state=RANDOM_STATE)),

        # Extra Trees: Similar ao Random Forest, mas com splits mais aleat√≥rios (reduz ainda mais a vari√¢ncia)
        ('ExtraTrees', ExtraTreesClassifier(n_estimators=50, class_weight='balanced', n_jobs=-1, random_state=RANDOM_STATE)),

        # AdaBoost: Boosting cl√°ssico, foca nos erros anteriores (bom para combinar com √°rvores simples)
        ('AdaBoost', AdaBoostClassifier(n_estimators=50, random_state=RANDOM_STATE)),
        
        # XGBoost: Estado da arte em boosting (r√°pido e perform√°tico). scale_pos_weight ajusta o desbalanceamento.
        ('XGBoost', XGBClassifier(eval_metric='logloss', scale_pos_weight=90, n_jobs=-1, random_state=RANDOM_STATE))
    ]

    # Tentativa de importar LightGBM (√≥timo para grandes volumes, mas requer instala√ß√£o extra)
    try:
        from lightgbm import LGBMClassifier
        models.append(('LightGBM', LGBMClassifier(class_weight='balanced', n_jobs=-1, random_state=RANDOM_STATE, verbose=-1)))
        print("‚úÖ LightGBM inclu√≠do no torneio.")
    except ImportError:
        print("‚ö†Ô∏è LightGBM n√£o encontrado. Pulando...")

    # Tentativa de importar CatBoost (Excelente com features categ√≥ricas e robusto a overfitting)
    try:
        from catboost import CatBoostClassifier
        # verbose=0 remove o output de treinamento
        models.append(('CatBoost', CatBoostClassifier(verbose=0, auto_class_weights='Balanced', random_state=RANDOM_STATE)))
        print("‚úÖ CatBoost inclu√≠do no torneio.")
    except ImportError:
        print("‚ö†Ô∏è CatBoost n√£o encontrado. Pulando (instale 'pip install catboost' para testar)...")

    # --------------------------------------------------------------------------
    # 4. CONFIGURA√á√ÉO DE M√âTRICAS E PIPELINE
    # Definimos quais m√©tricas queremos acompanhar. ROC_AUC √© a principal para classifica√ß√£o desbalanceada.
    # --------------------------------------------------------------------------
    scoring_metrics = {
        'recall': 'recall',       # Capacidade de encontrar TODAS as fraudes
        'precision': 'precision', # Capacidade de n√£o alertar alarmes falsos
        'f1': 'f1',               # Equil√≠brio entre Recall e Precision
        'roc_auc': 'roc_auc'      # Capacidade geral de separar classes (independente do threshold)
    }
    
    results_list = []
    
    # Buffer para o relat√≥rio textual
    report_buffer = [
        f"RELAT√ìRIO DE COMPARA√á√ÉO DE MODELOS",
        f"===================================",
        f"Amostra: {len(X_sample)} linhas",
        f"Folds: {CV_FOLDS}",
        f"Estrat√©gia: Preprocessamento -> SMOTE -> Modelo",
        f"-----------------------------------"
    ]

    print(f"\nüèÉ Rodando Cross-Validation ({CV_FOLDS} folds)...")
    
    # Recupera o pipeline de transformacao com Feature Engineering EDA-driven
    eda_engineer = EDAFeatureEngineer()
    X_engineered = eda_engineer.fit_transform(X_sample)
    preprocessor = get_preprocessor(X_engineered)

    for name, model in models:
        print(f"   >> Avaliando: {name}...", end=" ")
        
        # CRITICO: Pipeline com Imbalanced-Learn + Feature Engineering
        # O EDAFeatureEngineer aplica as melhorias do EDA (sentinelas, outliers, flags).
        # O SMOTE (criacao de dados sinteticos) deve ocorrer DENTRO do pipeline.
        # Isso garante que ele so veja os dados de TREINO do fold atual.
        pipeline = ImbPipeline(steps=[
            ('eda_features', eda_engineer),                # 0. Feature Engineering EDA-driven
            ('preprocessor', preprocessor),                # 1. Trata categoricas/numericas
            ('smote', SMOTE(random_state=RANDOM_STATE)),   # 2. Balanceia as classes artificialmente
            ('model', model)                               # 3. Treina o modelo
        ])
        
        # Executa a Valida√ß√£o Cruzada
        cv_results = cross_validate(
            pipeline, X_sample, y_sample, 
            cv=CV_FOLDS, scoring=scoring_metrics, 
            n_jobs=-1, return_train_score=False
        )
        
        # Processamento dos Resultados do Fold
        row = {'Model': name}
        txt_row = f"Model: {name: <15} | "
        
        for metric in scoring_metrics:
            mean_score = cv_results[f'test_{metric}'].mean()
            std_score = cv_results[f'test_{metric}'].std()
            
            row[f'{metric}_mean'] = mean_score
            row[f'{metric}_std'] = std_score
            txt_row += f"{metric.upper()}: {mean_score:.4f} (+/-{std_score:.4f})  "
        
        results_list.append(row)
        report_buffer.append(txt_row)
        print("Feito.")

    # --------------------------------------------------------------------------
    # 5. PERSIST√äNCIA DOS RESULTADOS
    # Salva os artefatos para an√°lise posterior.
    # --------------------------------------------------------------------------
    results_df = pd.DataFrame(results_list)
    results_df = results_df.sort_values(by='roc_auc_mean', ascending=False) # Ordena√ß√£o pelo ranking principal
    
    # Salva Tabela CSV (Dados Brutos)
    csv_path = COMPARISON_DATA_DIR / "models_comparison_results.csv"
    results_df.to_csv(csv_path, index=False)
    
    # Salva Relat√≥rio TXT (Formatado)
    report_buffer.append("\nRANKING FINAL (Ordenado por ROC_AUC):")
    report_buffer.append(results_df.to_string(index=False))
    
    with open(COMPARISON_REPORT_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(report_buffer))
        
    print(f"\nüíæ Resultados salvos em:")
    print(f"   - CSV: {csv_path}")
    print(f"   - TXT: {COMPARISON_REPORT_FILE}")

    # --------------------------------------------------------------------------
    # 6. VISUALIZA√á√ÉO (GR√ÅFICOS)
    # Cria gr√°fico de barras comparativo.
    # --------------------------------------------------------------------------
    # Transforma√ß√£o "melt" para formato longo, necess√°rio para o Seaborn plotar barras agrupadas
    metrics_to_plot = ['roc_auc_mean', 'recall_mean', 'precision_mean', 'f1_mean']
    
    plt.figure(figsize=(14, 8))
    melted = results_df.melt(id_vars="Model", value_vars=metrics_to_plot, var_name="Metric", value_name="Score")
    
    sns.barplot(data=melted, x="Model", y="Score", hue="Metric", palette="viridis")
    plt.title("Compara√ß√£o de M√©tricas por Modelo")
    plt.ylim(0, 1.05) # Eixo Y fixo entre 0 e 1 (j√° que s√£o porcentagens)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    
    img_path = FIGURES_DIR / "model_comparison_metrics.png"
    plt.savefig(img_path)
    print(f"   - Imagem: {img_path}")

    # --------------------------------------------------------------------------
    # 7.RECOMENDA√á√ÉO AUTOM√ÅTICA
    # Identifica o vencedor baseado puramente na m√©trica alvo (ROC-AUC).
    # --------------------------------------------------------------------------
    winner = results_df.iloc[0]
    print(f"\nüèÜ VENCEDOR GERAL: {winner['Model']} (ROC-AUC: {winner['roc_auc_mean']:.4f})")
    print(f"   Recomenda√ß√£o: Utilize o {winner['Model']} para a etapa de otimiza√ß√£o de hiperpar√¢metros.")

if __name__ == "__main__":
    compare_algorithms()